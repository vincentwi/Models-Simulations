{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-colombia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T02:18:32.877056Z",
     "start_time": "2022-06-09T02:18:32.873065Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy.stats.multivariate_normal import logpdf as jlogpdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-wyoming",
   "metadata": {},
   "source": [
    "# Question 1: Reading and formatting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 : Read the datset with pandas\n",
    "dataset = pd.read_csv(\"GermanCredit.txt\", sep =\"\\s+\", header=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 : Creating ytrain. For convenience, we will use 0 and 1 as labels\n",
    "dataset[24] = dataset[24] - 1\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Split into train and test set\n",
    "M = 800 # train set size\n",
    "d = 24\n",
    "length = dataset.shape[0]\n",
    "y_train = dataset.loc[:M-1, d]\n",
    "y_test = dataset.loc[M:, d]\n",
    "\n",
    "x_train = dataset.loc[:M-1, :d-1]\n",
    "x_test = dataset.loc[M:, :d-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 Center and scale the features\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 : scaling xtrain \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4 Extend them with a column of ones, which is for logistic regression\n",
    "ones_train = jnp.ones((M, 1))\n",
    "ones_test = jnp.ones((length-M, 1))\n",
    "x_train = jnp.concatenate((ones_train, x_train), axis=1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape changes to dimension 25\n",
    "x_test = jnp.concatenate((ones_test, x_test), axis=1)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that the values are scaled \n",
    "jnp.mean(x_train, axis=0), jnp.std(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-novel",
   "metadata": {},
   "source": [
    "# Question 2: Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-network",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "##### 2.1 : Proof of log-odds ratio\n",
    "Let us denote $$z= \\beta_0 + \\sum_{j = 1}^n \\beta_j X_j.$$\n",
    "\n",
    "We are to show that\n",
    "$$ \\log \\frac{P(Y=1|\\beta)}{1-P(Y=1|\\beta)} = z.$$\n",
    "We can rewrite the left hand side by splitting the fraction of the log\n",
    "\\begin{align*} \n",
    "\\log \\frac{P(Y=1|\\beta)}{1-P(Y=1|\\beta)} &=  \\log \\frac{1}{1+e^{-z}} - \\log \\frac{e^{-z}}{1+e^{-z}}\\\\ \n",
    " &=   \\log 1 - \\log e^{-z}\\\\ \n",
    " &=  - \\log e^{-z}\\\\ \n",
    " &= z\n",
    "\\end{align*}\n",
    "Note that in the second step we used that you can split again to 4 logarithms and then the log of the denominators will have opposite sign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-algeria",
   "metadata": {},
   "source": [
    "##### 2.2 : Parameters\n",
    "We can interepret these parameters as XYZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 : Decision Boundry. is it in terms of prob or vars >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-traffic",
   "metadata": {},
   "source": [
    "##### 2.4 : Proof of log-likelihood\n",
    "Let us denote $$z_i= \\beta_0 + \\sum_{j = 1}^n \\beta_j X_{i,j}.$$\n",
    "\n",
    "If $y_i = 1$, we see that the right hand side is \n",
    "\\begin{align*} \n",
    "\\log \\frac{1}{1+e^{-z_i}}  &= \\log \\frac{e^z_i}{1+e^{z_i}}\\\\\n",
    "&= z_i - \\log 1+e^z_i\\\\\n",
    "&= y_i z_i - \\log 1+e^z_i.\n",
    "\\end{align*}\n",
    "\n",
    "If $y_i = 0$, we see that the right hand side is \n",
    "\\begin{align*} \n",
    "\\log \\frac{e^{-z_i}}{1+e^{-z_i}}  &= \\log \\frac{1}{1+e^{z_i}}\\\\\n",
    "&= 0 - \\log 1+e^{z_i}\\\\\n",
    "&= y_i z_i - \\log 1+e^{z_i}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation – never used\n",
    "def log_likelihood(beta):\n",
    "    x_beta = np.matmul(x_train, beta)\n",
    "    output = np.sum(y_train * x_beta - np.log(1 + np.exp(x_beta)))\n",
    "    return output\n",
    "\n",
    "math needed: Is there an error?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-mexico",
   "metadata": {},
   "source": [
    "##### 2.5 : Jax-compatible log-likelihood\n",
    "\n",
    "We are asked to calculate\n",
    "$$\\log P(y_1,...,y_m| \\beta).$$\n",
    "\n",
    "By independence of the training data points, this equals\n",
    "$$\\log \\prod_{i=1}^m P(y_i| \\beta).$$\n",
    "\n",
    "We can now use the result from previous exercise to see that \n",
    "\n",
    "\\begin{align*} \n",
    "\\log \\prod_{i=1}^m P(y_i| \\beta) &=  \\sum_{i=1}^m \\log P(y_i| \\beta) \\\\\n",
    "&= \\sum_{i=1}^m (y_i z_i - \\log 1+e^z_i)\\\\\n",
    "&= \\sum_{i=1}^m y_i z_i -  \\sum_{i=1}^m \\log 1+e^z_i.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#math and speed comparison needed\n",
    "@jit\n",
    "def log_likelihood_jax(beta):\n",
    "    x_beta = jnp.matmul(x_train, beta)\n",
    "    output = jnp.sum(y_train * x_beta - jnp.log(1 + jnp.exp(x_beta)))\n",
    "    return output\n",
    "\n",
    "# After investigation, the jit implementations speeds up significantly\n",
    "jit_likelihood_jax = jit(log_likelihood_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6 : Gradient\n",
    "# speed comparison needed\n",
    "\n",
    "# Evaluates the gradient of the log likelihood for any beta\n",
    "# After investigation, the jit implementations speeds up significantly\n",
    "grad_log_likelihood = jit(grad(log_likelihood_jax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 : Logprior Function\n",
    "#speed comparison is needed\n",
    "\n",
    "DIM = 25\n",
    "constant = jnp.pi**2 * M / (3*DIM)\n",
    "Sigma = constant * jnp.linalg.inv(jnp.matmul(x_train.T, x_train))\n",
    "\n",
    "@jit\n",
    "def log_prior(beta):\n",
    "    '''\n",
    "    Input – beta: a vector of size d+1\n",
    "    Output – log prior density: constant\n",
    "    '''\n",
    "    return jlogpdf(beta, mean = jnp.zeros(DIM), cov = Sigma)\n",
    "\n",
    "\n",
    "# After investigation, the jit implementations does not speed up significantly\n",
    "# jitlogprior = jit(logprior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.8 : Gradient\n",
    "#speed comparison is needed\n",
    "grad_log_prior = jit(grad(log_prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7221565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.9 : Create the log posterior\n",
    "def log_posterior(beta):\n",
    "    '''\n",
    "    Input – beta: a vector of size d+1\n",
    "    Output – log posterior: constant\n",
    "    '''\n",
    "    return log_prior(beta) + log_likelihood(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.10 : Evaluates the gradient of the unnormalized log-posterior density\n",
    " \n",
    "def grad_log_posterior(beta):\n",
    "    '''\n",
    "    Input – beta: a vector of size d+1\n",
    "    Output – gradient step of log posterior: beta: a vector of size d+1\n",
    "    '''\n",
    "    return grad_log_prior(beta) + grad_log_likelihood(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-night",
   "metadata": {},
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: independent Metropolis-Hastings\n",
    "def sample_prior():\n",
    "    return multivariate_normal.rvs(mean=np.zeros(DIM), cov=Sigma)\n",
    "\n",
    "n_accept = 0\n",
    "N = 10000\n",
    "current_beta = sample_prior()\n",
    "store_beta = np.zeros((N, DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the loop\n",
    "for n in range(N):\n",
    "    #sample a proposed state\n",
    "    proposed_beta = sample_prior()\n",
    "\n",
    "    #evaluate posterior density\n",
    "    log_posterior_proposed = log_posterior(proposed_beta)\n",
    "    log_posterior_current = log_posterior(current_beta)\n",
    "\n",
    "    #evaluate transition likelihood\n",
    "    log_transition_proposed = log_prior(proposed_beta)\n",
    "    log_transition_current = log_prior(current_beta)\n",
    "    \n",
    "    #log acceptance prob\n",
    "    log_accept_prob = (log_posterior_proposed + log_transition_current\n",
    "                       - log_posterior_current - log_transition_proposed)\n",
    "\n",
    "    #accept tor reject\n",
    "    uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "    if np.log(uniform) < log_accept_prob:\n",
    "        current_beta = proposed_beta.copy() #accept\n",
    "        n_accept += 1\n",
    "\n",
    "    store_beta[n,:] = current_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82675dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Acceptance rate: \", n_accept/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = np.arange(1,N+1)\n",
    "plt.figure()\n",
    "plt.plot(iteration, store_beta[:,0])\n",
    "plt.plot(iteration, store_beta[:,1])\n",
    "plt.plot(iteration, store_beta[:,2])\n",
    "plt.plot(iteration, store_beta[:,3])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('beta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 : Random Walk Metropolis–Hastings algorithm\n",
    "s = 0.02\n",
    "SIG = jnp.eye(DIM) * s**2\n",
    "rng = jax.random.PRNGKey(0)\n",
    "n_accept = 0\n",
    "store_beta = np.zeros((N,DIM))\n",
    "beta = sample_prior()\n",
    "\n",
    "for n in range(N):\n",
    "#     epsilon = jax.random.multivariate_normal(key=rng, mean= jnp.zeros(DIM), cov=SIG)  \n",
    "    epsilon = np.random.multivariate_normal(mean= jnp.zeros(DIM), cov=SIG)  \n",
    "\n",
    "    proposed_state = beta + epsilon\n",
    "    \n",
    "    pi_y = log_posterior(proposed_state)\n",
    "    pi_x = log_posterior(beta)\n",
    "    \n",
    "    logacceptprob = float(pi_y - pi_x)\n",
    "    \n",
    "    #accept tor reject\n",
    "    uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "    if np.log(uniform) < logacceptprob:\n",
    "        beta = proposed_state.copy() #accept\n",
    "        n_accept += 1\n",
    "    store_beta[n,:] = beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_accept/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = np.arange(1,N+1)\n",
    "plt.figure()\n",
    "plt.plot(iteration, store_beta[:,0])\n",
    "plt.plot(iteration, store_beta[:,1])\n",
    "plt.plot(iteration, store_beta[:,2])\n",
    "plt.plot(iteration, store_beta[:,3])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('beta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = np.arange(1,N+1)\n",
    "plt.figure()\n",
    "plt.plot(iteration, np.cumsum(store_beta[:,0])/iteration)\n",
    "plt.plot(iteration, np.cumsum(store_beta[:,1])/iteration)\n",
    "plt.plot(iteration, np.cumsum(store_beta[:,2])/iteration)\n",
    "plt.plot(iteration, np.cumsum(store_beta[:,3])/iteration)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('beta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-correlation function\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plt.figure()\n",
    "plot_acf(store_beta[2000:,0], lags = 30, alpha = None)\n",
    "plot_acf(store_beta[2000:,1], lags = 30, alpha = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 : Metropolis-adjusted Langevin algorithm\n",
    "import scipy\n",
    "s = 0.08\n",
    "SIG = jnp.eye(DIM) * s**2\n",
    "rng = jax.random.PRNGKey(0)\n",
    "n_accept = 0\n",
    "store_beta = np.zeros((N,DIM))\n",
    "beta = sample_prior()\n",
    "\n",
    "for n in range(N):\n",
    "#     epsilon = jax.random.multivariate_normal(key=rng, mean= jnp.zeros(DIM), cov=SIG)    \n",
    "    epsilon = np.random.multivariate_normal(mean= jnp.zeros(DIM), cov=SIG)    \n",
    "    \n",
    "    proposed_state = beta + s**2 /2 * grad_log_posterior(beta) + epsilon\n",
    "    \n",
    "    pi_y = log_posterior(proposed_state)\n",
    "    pi_x = log_posterior(beta)\n",
    "    q_y = jlogpdataset(proposed_state, mean = beta + s**2 /2 \\\n",
    "                                                 * gradlogdensity(beta) , cov=SIG)\n",
    "    q_x = jlogpdataset(beta , mean = proposed_state + s**2 /2 \\\n",
    "                                                 * gradlogdensity(proposed_state) , cov=SIG)\n",
    "    \n",
    "    \n",
    "    \n",
    "    logacceptprob = float(pi_y + q_x - pi_x - q_y)\n",
    "    \n",
    "    #accept tor reject\n",
    "    uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "    if np.log(uniform) < logacceptprob:\n",
    "        beta = proposed_state.copy() #accept\n",
    "        n_accept += 1\n",
    "    store_beta[n,:] = beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_accept/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = np.arange(1,N+1)\n",
    "plt.figure()\n",
    "plt.plot(iteration, store_beta[:,0])\n",
    "plt.plot(iteration, store_beta[:,1])\n",
    "plt.plot(iteration, store_beta[:,2])\n",
    "plt.plot(iteration, store_beta[:,3])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('beta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = np.arange(1,N+1)\n",
    "plt.figure()\n",
    "plt.plot(iteration, np.cumsum(store_beta[:,0])/iteration)\n",
    "plt.plot(iteration, np.cumsum(store_beta[:,1])/iteration)\n",
    "plt.plot(iteration, np.cumsum(store_beta[:,2])/iteration)\n",
    "plt.plot(iteration, np.cumsum(store_beta[:,3])/iteration)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('beta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-correlation function\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plt.figure()\n",
    "plot_acf(store_beta[2000:,0], lags = 30, alpha = None)\n",
    "plot_acf(store_beta[2000:,1], lags = 30, alpha = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.4 : Hamiltonian Monte Carlo algorithm\n",
    "import numpy as np\n",
    "\n",
    "s = 0.08\n",
    "SIG = jnp.eye(DIM) * s**2\n",
    "n_accept = 0\n",
    "store_beta = np.zeros((N,DIM))\n",
    "beta = sample_prior()\n",
    "\n",
    "def hamiltonian_dynamics(current_state, current_velocity, stepsize, num_steps, gradlogdensity):\n",
    "    x = current_state\n",
    "    v = current_velocity\n",
    "    \"\"\"Simulate Hamiltonian dynamics.\"\"\"    \n",
    "    v = v + stepsize * np.array(grad_log_posterior(x)) / 2\n",
    "    for step in range(num_steps): \n",
    "        x = x + stepsize * v \n",
    "        if step != (num_steps-1):\n",
    "            v = v + stepsize * np.array(grad_log_posterior(x))\n",
    "            v = v + stepsize * np.array(grad_log_posterior(x)) / 2    \n",
    "    return (x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamiltonian_dynamics(beta, beta, s, N, gradlogdensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-entity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-pasta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-coordinator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "expected-semiconductor",
   "metadata": {},
   "source": [
    "# Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-century",
   "metadata": {},
   "source": [
    "##### 4.1\n",
    "Under the integral, we can see the expit function and a probability distribution. To estimate this integral, we sample $N$ points $\\{ \\beta^{(t)}, \\space t \\in \\{1, .. , N\\} \\space \\}$ from the distribution $P(\\beta | y_1,..,y_m)$ and then add the obtained values $$\\text{expit}(z^{(t)})$$ where \n",
    "$$z^{(t)}= \\beta_0^{(t)} + \\sum_{j = 1}^n \\beta_j^{(t)} X_{j}.$$\n",
    "We give every point $\\beta^{(t)}$ a weight of $1/N$ so that the total weight is 1 and equals the integral of the probability function. This way, we obtain the method of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 : Approximated predictive probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 : Prediction rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.4 : Misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 : Cost function\n",
    "\n",
    "def adjust(y_test, y_pred):\n",
    "    '''\n",
    "    Input – y_test: vector with good/bad credit risk\n",
    "          – y_pred: pred values for good/bad credit risk\n",
    "    Output – average cost – constat'''\n",
    "    f = lamda x,y:  5 if x==0 and y==1\\\n",
    "                   else 1 if x==1 and y==0 \\\n",
    "                    else 0\n",
    "    vf = np.vectorize(f)\n",
    "    return vf(y_test, y_pred)\n",
    "\n",
    "\n",
    "def average_cost(y_test):\n",
    "    '''\n",
    "    Input – y_test: vector with good/bad credit risk\n",
    "    Output – average cost – constat''' \n",
    "    return jnp.sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.6 : Maximum Likelihood Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.7 : Misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.8 : Prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8fc5084e69b64ba3a8305ae1e10bbd89152d233669d0e5f5d19e1a4da7e5633"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
