{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-colombia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:15:46.933821Z",
     "start_time": "2022-06-13T09:15:28.913962Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.scipy.stats import multivariate_normal\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-wyoming",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Question 1: Reading and formatting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-anatomy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:27.797675Z",
     "start_time": "2022-06-13T09:16:27.758838Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.1 : Read the datset with pandas\n",
    "dataset = pd.read_csv(\"GermanCredit.txt\", sep =\"\\s+\", header=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-accounting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:27.899892Z",
     "start_time": "2022-06-13T09:16:27.801670Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.2 : Creating ytrain. For convenience, we will use 0 and 1 as labels\n",
    "dataset[24] = dataset[24] - 1\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-discussion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:27.915835Z",
     "start_time": "2022-06-13T09:16:27.908256Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1.2 Split into train and test set\n",
    "M = 800 # train set size\n",
    "d = 24\n",
    "length = dataset.shape[0]\n",
    "y_train = dataset.loc[:M-1, d].to_numpy()\n",
    "y_test = dataset.loc[M:, d].to_numpy()\n",
    "\n",
    "x_train = dataset.loc[:M-1, :d-1]\n",
    "x_test = dataset.loc[M:, :d-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-natural",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:28.990406Z",
     "start_time": "2022-06-13T09:16:27.921144Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.3 Center and scale the features\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-setting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:29.014220Z",
     "start_time": "2022-06-13T09:16:28.993235Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.3 : scaling xtrain \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-galaxy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:29.457959Z",
     "start_time": "2022-06-13T09:16:29.018861Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.4 Extend them with a column of ones, which is for logistic regression\n",
    "ones_train = jnp.ones((M, 1))\n",
    "ones_test = jnp.ones((length-M, 1))\n",
    "x_train = jnp.concatenate((ones_train, x_train), axis=1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-ridge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:29.518927Z",
     "start_time": "2022-06-13T09:16:29.464446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# the shape changes to dimension 25\n",
    "x_test = jnp.concatenate((ones_test, x_test), axis=1)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-burning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:29.717952Z",
     "start_time": "2022-06-13T09:16:29.528534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#make sure that the values are scaled \n",
    "jnp.mean(x_train, axis=0), jnp.std(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-novel",
   "metadata": {},
   "source": [
    "# Question 2: Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-network",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:14:33.736487Z",
     "start_time": "2022-06-09T07:14:33.728079Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Proof of log-odds ratio\n",
    "Let us denote $$z= \\beta_0 + \\sum_{j = 1}^n \\beta_j X_j.$$\n",
    "\n",
    "We are to show that\n",
    "$$ \\log \\frac{P(Y=1|\\beta)}{1-P(Y=1|\\beta)} = z.$$\n",
    "We can rewrite the left hand side by splitting the fraction of the log\n",
    "\\begin{align*} \n",
    "\\log \\frac{P(Y=1|\\beta)}{1-P(Y=1|\\beta)} &=  \\log \\frac{1}{1+e^{-z}} - \\log \\frac{e^{-z}}{1+e^{-z}}\\\\ \n",
    " &=   \\log 1 - \\log e^{-z}\\\\ \n",
    " &=  - \\log e^{-z}\\\\ \n",
    " &= z\n",
    "\\end{align*}\n",
    "Note that in the second step we used that you can split again to 4 logarithms and then the log of the denominators will have opposite sign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291dec16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:14:33.751529Z",
     "start_time": "2022-06-09T07:14:33.745122Z"
    }
   },
   "source": [
    "## Interpreting the parameters\n",
    "These parameters represent the importance of features $X = (X_1,...,X_d)$ when we decide how likely $Y=1$(the individual having bad credit risk) compared to $Y=0$(the individual having good credit risk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaaab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:14:33.779556Z",
     "start_time": "2022-06-09T07:14:33.764114Z"
    }
   },
   "source": [
    "## Decision Boundry\n",
    "When $\\beta_0 + \\sum_{j = 1}^n \\beta_j X_j$ is positive, it means the individual has a bad credit risk.Otherwise, he has a good credit risk. After fitting it into sigmoid function, the threshold will be 0.5, because $expit(0) = \\log \\frac{1}{1+e^{0}} = 0.5 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-traffic",
   "metadata": {},
   "source": [
    "## Proof of log-likelihood\n",
    "Let us denote $$z_i= \\beta_0 + \\sum_{j = 1}^n \\beta_j X_{i,j}.$$\n",
    "\n",
    "If $y_i = 1$, we see that the right hand side is \n",
    "\\begin{align*} \n",
    "\\log \\frac{1}{1+e^{-z_i}}  &= \\log \\frac{e^z_i}{1+e^{z_i}}\\\\\n",
    "&= z_i - \\log 1+e^z_i\\\\\n",
    "&= y_i z_i - \\log 1+e^z_i.\n",
    "\\end{align*}\n",
    "\n",
    "If $y_i = 0$, we see that the right hand side is \n",
    "\\begin{align*} \n",
    "\\log \\frac{e^{-z_i}}{1+e^{-z_i}}  &= \\log \\frac{1}{1+e^{z_i}}\\\\\n",
    "&= 0 - \\log 1+e^{z_i}\\\\\n",
    "&= y_i z_i - \\log 1+e^{z_i}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-mexico",
   "metadata": {},
   "source": [
    "## JAX-compatible log-likelihood\n",
    "\n",
    "We are asked to calculate\n",
    "$$\\log P(y_1,...,y_m| \\beta).$$\n",
    "\n",
    "By independence of the training data points, this equals\n",
    "$$\\log \\prod_{i=1}^m P(y_i| \\beta).$$\n",
    "\n",
    "We can now use the result from previous exercise to see that \n",
    "\n",
    "\\begin{align*} \n",
    "\\log \\prod_{i=1}^m P(y_i| \\beta) &=  \\sum_{i=1}^m \\log P(y_i| \\beta) \\\\\n",
    "&= \\sum_{i=1}^m (y_i z_i - \\log 1+e^z_i)\\\\\n",
    "&= \\sum_{i=1}^m y_i z_i -  \\sum_{i=1}^m \\log 1+e^z_i.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-conference",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:29.732927Z",
     "start_time": "2022-06-13T09:16:29.727832Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Write log-likelihood function in numpy\n",
    "# def log_likelihood(beta):\n",
    "#     x_beta = np.matmul(x_train, beta)\n",
    "#     output = np.sum(y_train * x_beta - np.log(1 + np.exp(x_beta)))\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-transformation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:29.754915Z",
     "start_time": "2022-06-13T09:16:29.739187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write log-likelihood function in a JAX-compatible way\n",
    "@jit\n",
    "def log_likelihood_jax(beta):\n",
    "    x_beta = jnp.matmul(x_train, beta)\n",
    "    output = jnp.sum(y_train * x_beta - jnp.log(1 + jnp.exp(x_beta)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a53e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:33.139010Z",
     "start_time": "2022-06-13T09:16:29.768060Z"
    }
   },
   "outputs": [],
   "source": [
    "# To compare the speed with or without jit\n",
    "import time\n",
    "\n",
    "# simulate an array of beta\n",
    "beta = np.random.rand(25)\n",
    "jit_loglikelihood_jax = jit(log_likelihood_jax)\n",
    "%timeit jit_loglikelihood_jax(beta)\n",
    "%timeit log_likelihood_jax(beta)\n",
    "\n",
    "## There is no significant speedup, we won't use jit in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c19436e",
   "metadata": {},
   "source": [
    "## Gradient of the log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-wrestling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:34.965826Z",
     "start_time": "2022-06-13T09:16:33.146778Z"
    }
   },
   "outputs": [],
   "source": [
    "gradloglikelihood1 = grad(log_likelihood_jax) # without JIT\n",
    "gradloglikelihood2 = jit(grad(log_likelihood_jax)) # with JIT \n",
    "\n",
    "# for the gradient, there is a signfiicant speedup with JIT\n",
    "%timeit gradloglikelihood1(beta)\n",
    "%timeit gradloglikelihood2(beta)\n",
    "\n",
    "# After investigation, the jit implementations speeds up significantly. Use the faster implementation\n",
    "gradloglikelihood = jit(grad(log_likelihood_jax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89afd6",
   "metadata": {},
   "source": [
    "## Log-prior density evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-leone",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:35.032745Z",
     "start_time": "2022-06-13T09:16:34.977023Z"
    }
   },
   "outputs": [],
   "source": [
    "DIM = 25\n",
    "constant = np.pi**2 * M / (3*DIM)\n",
    "Sigma = constant * np.linalg.inv(np.matmul(x_train.T, x_train))\n",
    "\n",
    "@jit\n",
    "def log_prior(beta):\n",
    "    '''\n",
    "    Input – beta: a vector of size d+1\n",
    "    Output – log prior density: constant\n",
    "    '''\n",
    "    return multivariate_normal.logpdf(beta, mean=jnp.zeros(DIM), cov=Sigma)\n",
    "\n",
    "\n",
    "# After investigation, the jit implementations does not speed up significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a790cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:16:59.783766Z",
     "start_time": "2022-06-13T09:16:35.035190Z"
    }
   },
   "outputs": [],
   "source": [
    "log_prior1 = log_prior # without JIT\n",
    "log_prior2 = jit(log_prior) # with JIT \n",
    "%timeit log_prior1(beta)\n",
    "%timeit log_prior2(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da846cd",
   "metadata": {},
   "source": [
    "## Gradient of the log-prior evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-disability",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:17:13.846280Z",
     "start_time": "2022-06-13T09:16:59.807138Z"
    }
   },
   "outputs": [],
   "source": [
    "grad_log_prior1 = grad(log_prior)\n",
    "grad_log_prior2 = jit(grad(log_prior))\n",
    "\n",
    "%timeit grad_log_prior1(beta)\n",
    "%timeit grad_log_prior2(beta)\n",
    "\n",
    "# After investigation, jit speeds up gradient computation significantly\n",
    "grad_log_prior = grad_log_prior2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee9c7e",
   "metadata": {},
   "source": [
    "## Unnormalized log-posterior density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7221565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:17:13.855199Z",
     "start_time": "2022-06-13T09:17:13.850112Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_posterior(beta):\n",
    "    '''\n",
    "    Input – beta: a vector of size d+1\n",
    "    Output – log posterior: constant\n",
    "    '''\n",
    "    return log_prior(beta) + log_likelihood_jax(beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcdd014",
   "metadata": {},
   "source": [
    "## Gradient of log-posterior density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68d8f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:17:13.905316Z",
     "start_time": "2022-06-13T09:17:13.859424Z"
    }
   },
   "outputs": [],
   "source": [
    "def grad_log_posterior(beta):\n",
    "    '''\n",
    "    Input – beta: a vector of size d+1\n",
    "    Output – gradient step of log posterior: beta: a vector of size d+1.\n",
    "    '''\n",
    "    return grad_log_prior(beta) + gradloglikelihood(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-night",
   "metadata": {},
   "source": [
    "# Markov chain Monte Carlo sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e4500",
   "metadata": {},
   "source": [
    "## Independent Metropolis–Hastings algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc9fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:17:25.177057Z",
     "start_time": "2022-06-13T09:17:13.913898Z"
    }
   },
   "outputs": [],
   "source": [
    "# since jax's random generation is painful, we will switch to standard scipy\n",
    "import scipy\n",
    "def sample_prior():\n",
    "    return scipy.stats.multivariate_normal.rvs(mean=np.zeros(DIM), cov=Sigma)\n",
    "\n",
    "n_accept = 0\n",
    "N = 10000\n",
    "\n",
    "# Initialize the Markov Chain\n",
    "current_beta = sample_prior()\n",
    "store_beta = np.zeros((N, DIM))\n",
    "for n in range(N):\n",
    "    # sample a proposed state\n",
    "    proposed_beta = sample_prior()\n",
    "\n",
    "    # evaluate posterior density\n",
    "    log_posterior_proposed = log_posterior(proposed_beta)\n",
    "    log_posterior_current = log_posterior(current_beta)\n",
    "\n",
    "    # evaluate transition likelihood\n",
    "    log_transition_proposed = log_prior(proposed_beta)\n",
    "    log_transition_current = log_prior(current_beta)\n",
    "    \n",
    "    # log acceptance prob\n",
    "    log_accept_prob = (log_posterior_proposed + log_transition_current\n",
    "                       - log_posterior_current - log_transition_proposed)\n",
    "\n",
    "    # accept or reject\n",
    "    uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "    if np.log(uniform) < log_accept_prob:\n",
    "        current_beta = proposed_beta.copy() #accept\n",
    "        n_accept += 1\n",
    "    # store states\n",
    "    store_beta[n,:] = current_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82675dc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:17:25.486689Z",
     "start_time": "2022-06-13T09:17:25.179277Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Acceptance rate: \", n_accept/N)\n",
    "\n",
    "iteration = np.arange(1, N+1)\n",
    "plt.figure()\n",
    "plt.plot(iteration, store_beta[:,0])\n",
    "plt.plot(iteration, store_beta[:,1])\n",
    "plt.plot(iteration, store_beta[:,2])\n",
    "plt.plot(iteration, store_beta[:,3])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('beta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5c605",
   "metadata": {},
   "source": [
    "<mark> Write down some comments on the performance of the algorithm using diagnostic plots.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bd9ac",
   "metadata": {},
   "source": [
    "## A random walk Metropolis–Hastings algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-output",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:17:53.404096Z",
     "start_time": "2022-06-13T09:17:25.491208Z"
    }
   },
   "outputs": [],
   "source": [
    "#3.2 Experiment with the choice of sigma\n",
    "sigma_list = [0.002, 0.02, 0.2]\n",
    "for sigma in sigma_list:\n",
    "    noise_variance = np.identity(DIM) * sigma**2\n",
    "    n_accept = 0\n",
    "    current_beta = sample_prior()\n",
    "    store_beta = np.zeros((N, DIM))\n",
    "\n",
    "    for n in range(N):\n",
    "        noise = scipy.stats.multivariate_normal.rvs(mean=np.zeros(DIM),\n",
    "                                       cov=noise_variance)\n",
    "        proposed_beta = current_beta + noise\n",
    "\n",
    "        #evaluate posterior density\n",
    "        log_posterior_proposed = log_posterior(proposed_beta)\n",
    "        log_posterior_current = log_posterior(current_beta)\n",
    "\n",
    "        #accept or reject\n",
    "        log_accept_prob = log_posterior_proposed - log_posterior_current\n",
    "        uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "        if np.log(uniform) < log_accept_prob:\n",
    "            current_beta = proposed_beta.copy() #accept\n",
    "            n_accept += 1\n",
    "\n",
    "        store_beta[n,:] = current_beta\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(iteration, store_beta[:,1])\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('beta')\n",
    "    plt.title(\"Evolution of beta\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_acf(store_beta[:,1], alpha=None)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xlabel('lag')\n",
    "    plt.ylabel('autocorrelation')\n",
    "    plt.show()\n",
    "    print(f\"Acceptance probability is {n_accept/N}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e4de4",
   "metadata": {},
   "source": [
    "Based on autocorrelation graph, we can tell sigma=0.02 is best among three as it drops down to 0 faster than two others. However, it is not optimal as it's still far away from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff48bd",
   "metadata": {},
   "source": [
    "## Metropolis-adjusted Langevin algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00d739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:20:17.110690Z",
     "start_time": "2022-06-13T09:17:53.408479Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigma_list = [0.04, 0.08, 0.12]\n",
    "for sigma in sigma_list:\n",
    "    n_accept = 0\n",
    "    current_beta = sample_prior()\n",
    "    store_beta = np.zeros((N, DIM))\n",
    "    noise_variance = np.identity(DIM) * sigma**2\n",
    "    for n in range(N):\n",
    "        noise = scipy.stats.multivariate_normal.rvs(mean=np.zeros(DIM),\n",
    "                                       cov=noise_variance)\n",
    "        proposed_beta = (current_beta +\n",
    "                         0.5 * sigma**2 * grad_log_posterior(current_beta) +\n",
    "                         noise)\n",
    "\n",
    "        #evaluate posterior density\n",
    "        log_posterior_proposed = log_posterior(proposed_beta)\n",
    "        log_posterior_current = log_posterior(current_beta)\n",
    "        log_transition_proposed = multivariate_normal.logpdf(\n",
    "            proposed_beta,\n",
    "            mean=current_beta\n",
    "                 + 0.5 * sigma**2 * grad_log_posterior(current_beta),\n",
    "            cov=noise_variance)\n",
    "        log_transition_current = scipy.stats.multivariate_normal.logpdf(\n",
    "            current_beta,\n",
    "            mean=proposed_beta\n",
    "                 + 0.5 * sigma**2 * grad_log_posterior(proposed_beta),\n",
    "            cov=noise_variance)\n",
    "\n",
    "        #accept or reject\n",
    "        log_accept_prob = (log_posterior_proposed\n",
    "                           + log_transition_current\n",
    "                           - log_posterior_current\n",
    "                           - log_transition_proposed)\n",
    "        uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "        if np.log(uniform) < log_accept_prob:\n",
    "            current_beta = proposed_beta.copy() #accept\n",
    "            n_accept += 1\n",
    "\n",
    "        store_beta[n,:] = current_beta\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(iteration, store_beta[:,1])\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('beta')\n",
    "    plt.title(\"Evolution of beta\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_acf(store_beta[:,1], alpha=None)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xlabel('lag')\n",
    "    plt.ylabel('autocorrelation')\n",
    "    plt.show()\n",
    "    print(f\"Acceptance probability is {n_accept/N}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4201e",
   "metadata": {},
   "source": [
    "The sigma=0.08 seems to be the best choice. First and second choices have comparable performance on diagnostic plots. The trace plots of beta show stationarity while the second choice shows a faster drop on autocorrelation. Considering the balance between large move and decent acceptance ratio, sigma=0.08 performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13635203",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte Carlo algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-steal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:29:26.711629Z",
     "start_time": "2022-06-13T09:22:36.313098Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#3.4 : Hamiltonian Monte Carlo algorithm\n",
    "from hmc import hamiltonian_dynamics\n",
    "step_size_list = [0.01, 0.05, 0.1]\n",
    "number_step_list = [5,10]\n",
    "\n",
    "for step_size in step_size_list:\n",
    "    for number_step in number_step_list:\n",
    "        n_accept = 0\n",
    "        current_beta = sample_prior()\n",
    "        store_beta = np.zeros((N, DIM))\n",
    "        for n in range(N):\n",
    "            velocity = scipy.stats.multivariate_normal.rvs(mean=np.zeros(DIM), cov=np.identity(DIM))\n",
    "            proposed_beta, proposed_velocity = hamiltonian_dynamics(\n",
    "                current_beta, velocity, step_size, number_step, grad_log_posterior)\n",
    "            log_posterior_proposed = log_posterior(proposed_beta)\n",
    "            log_posterior_current = log_posterior(current_beta)\n",
    "            log_prob_proposed = multivariate_normal.logpdf(\n",
    "                proposed_velocity, mean=np.zeros(DIM), cov=np.identity(DIM))\n",
    "            log_prob_current = multivariate_normal.logpdf(\n",
    "                velocity, mean=np.zeros(DIM), cov=np.identity(DIM))\n",
    "\n",
    "            #accept or reject\n",
    "            log_accept_prob = (log_posterior_proposed\n",
    "                               + log_prob_proposed\n",
    "                               - log_posterior_current\n",
    "                               - log_prob_current)\n",
    "            uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "            if np.log(uniform) < log_accept_prob:\n",
    "                current_beta = proposed_beta.copy() #accept\n",
    "                n_accept += 1\n",
    "\n",
    "            store_beta[n,:] = current_beta\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iteration, store_beta[:,1])\n",
    "        plt.xlabel('iteration')\n",
    "        plt.ylabel('beta')\n",
    "        plt.title(\"Evolution of beta\")\n",
    "        plt.show()\n",
    "\n",
    "        plot_acf(store_beta[:,1], alpha=None)\n",
    "        plt.ylim([0, 1.1])\n",
    "        plt.xlabel('lag')\n",
    "        plt.ylabel('autocorrelation')\n",
    "        plt.show()\n",
    "        print(f\"Step size of {step_size} with {number_step} steps\")\n",
    "        print(f\"Acceptance probability is {n_accept/N}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26870ce8",
   "metadata": {},
   "source": [
    "Based on diagnostic plots and acceptance ratios, step size of 0.05 with 5 steps is the best setup as the autocorrelation plumps to 0 very fast and trace plot shows quite stable stationarity. Meanwhile, 0.8513 acceptance ratio is decent and 0.05 as step size can ensure a larger move in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a final run of the Markov chain, with the best stepsize that you found and 10 number of steps, \n",
    "# for 11, 000 iterations and discard the first 1000 iterations as burn-in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-century",
   "metadata": {},
   "source": [
    "##### 4.1\n",
    "Under the integral, we can see the expit function and a probability distribution. To estimate this integral, we sample $N$ points $\\{ \\beta^{(t)}, \\space t \\in \\{1, .. , N\\} \\space \\}$ from the distribution $P(\\beta | y_1,..,y_m)$ and then add the obtained values $$\\text{expit}(z^{(t)})$$ where \n",
    "$$z^{(t)}= \\beta_0^{(t)} + \\sum_{j = 1}^n \\beta_j^{(t)} X_{j}.$$\n",
    "We give every point $\\beta^{(t)}$ a weight of $1/N$ so that the total weight is 1 and equals the integral of the probability function. This way, we obtain the method of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-seminar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:20:17.447109Z",
     "start_time": "2022-06-13T09:16:27.824Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.2 : Approximated predictive probabilities\n",
    "\n",
    "# 4.2\n",
    "# we need \\beta^t sets from 3.4 -> Call them store_beta .. np.array () row is person\n",
    "# We need x_test, which are the data we will try to predict\n",
    "n = 200 # number of test ppl\n",
    "def predict_one(x_test_person, store_beta):\n",
    "    total_sum = 0\n",
    "    for i in range(N): #range over store_beta\n",
    "        z = np.dot(store_beta[i], x_test_person)\n",
    "        expitz = (1+ np.exp(-z))**-1\n",
    "        total_sum += expitz /N\n",
    "    return total_sum\n",
    "    \n",
    "def predict_all_probs(x_test, store_beta):\n",
    "    solutions = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        solutions[i] = predict_one(x_test[i], store_beta)\n",
    "    return solutions\n",
    "\n",
    "\n",
    "\n",
    "#4.3 : Prediction rule\n",
    "# if predict gives a probability smaller than 0.5, return 0,\n",
    "# if it is larger than 0.5, return 1\n",
    "\n",
    "def predict_all(x_test, store_beta):\n",
    "    predictions_probs = predict_all_probs(x_test, store_beta)\n",
    "    f = lambda x:  1 if (x>0.5) else 0\n",
    "\n",
    "    solutions = [ f(x) for x in predictions_probs]\n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-prime",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:20:17.449178Z",
     "start_time": "2022-06-13T09:16:27.827Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = predict_all(x_test, store_beta)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-infection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:20:17.451212Z",
     "start_time": "2022-06-13T09:16:27.830Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.4 : Misclassification rate\n",
    "misclassification = np.mean(y_pred != y_test)\n",
    "misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-patient",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:20:17.454033Z",
     "start_time": "2022-06-13T09:16:27.833Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4.5 : Cost function\n",
    "def average_cost(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Input – y_test: vector with good/bad credit risk\n",
    "          – y_pred: pred values for good/bad credit risk\n",
    "    Output – average cost: constant\n",
    "    \"\"\"\n",
    "    cost_vector = np.zeros_like(y_test)\n",
    "    cost_vector[(y_pred == 0) & (y_test == 1)] = 5\n",
    "    cost_vector[(y_pred == 1) & (y_test == 0)] = 1\n",
    "    return np.mean(cost_vector)\n",
    "\n",
    "avg_cost = average_cost(y_test, y_pred)\n",
    "avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182964e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:20:17.458301Z",
     "start_time": "2022-06-13T09:16:27.835Z"
    }
   },
   "outputs": [],
   "source": [
    "adjusted_cost =adjust(y_test, y_pred)\n",
    "\n",
    "avg_cost = average_cost(adjusted_cost)\n",
    "avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-report",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:20:17.460632Z",
     "start_time": "2022-06-13T09:16:27.839Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.6 : Maximum Likelihood Estimator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-shipping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:20:17.462976Z",
     "start_time": "2022-06-13T09:16:27.841Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.7 : Misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-honduras",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T09:20:17.480399Z",
     "start_time": "2022-06-13T09:16:27.843Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.8 : Prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8fc5084e69b64ba3a8305ae1e10bbd89152d233669d0e5f5d19e1a4da7e5633"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}