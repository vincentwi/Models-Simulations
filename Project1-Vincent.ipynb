{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-colombia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:31.171640Z",
     "start_time": "2022-06-13T10:13:27.223480Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.scipy.stats import multivariate_normal\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-wyoming",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Question 1: Reading and formatting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-anatomy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:31.227369Z",
     "start_time": "2022-06-13T10:13:31.174366Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.1 : Read the datset with pandas\n",
    "dataset = pd.read_csv(\"GermanCredit.txt\", sep =\"\\s+\", header=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-accounting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:31.278925Z",
     "start_time": "2022-06-13T10:13:31.233554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.2 : Creating ytrain. For convenience, we will use 0 and 1 as labels\n",
    "dataset[24] = dataset[24] - 1\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-discussion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:31.291837Z",
     "start_time": "2022-06-13T10:13:31.284426Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1.2 Split into train and test set\n",
    "M = 800 # train set size\n",
    "d = 24\n",
    "length = dataset.shape[0]\n",
    "y_train = dataset.loc[:M-1, d].to_numpy()\n",
    "y_test = dataset.loc[M:, d].to_numpy()\n",
    "\n",
    "x_train = dataset.loc[:M-1, :d-1]\n",
    "x_test = dataset.loc[M:, :d-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-natural",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:31.865082Z",
     "start_time": "2022-06-13T10:13:31.295113Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.3 Center and scale the features\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-setting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:31.894958Z",
     "start_time": "2022-06-13T10:13:31.868229Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.3 : scaling xtrain \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-galaxy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:32.022455Z",
     "start_time": "2022-06-13T10:13:31.897844Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#1.4 Extend them with a column of ones, which is for logistic regression\n",
    "ones_train = jnp.ones((M, 1))\n",
    "ones_test = jnp.ones((length-M, 1))\n",
    "x_train = jnp.concatenate((ones_train, x_train), axis=1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-ridge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:32.061639Z",
     "start_time": "2022-06-13T10:13:32.028616Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# the shape changes to dimension 25\n",
    "x_test = jnp.concatenate((ones_test, x_test), axis=1)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-burning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:32.208353Z",
     "start_time": "2022-06-13T10:13:32.066109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#make sure that the values are scaled \n",
    "jnp.mean(x_train, axis=0), jnp.std(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-novel",
   "metadata": {},
   "source": [
    "# Question 2: Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-network",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:14:33.736487Z",
     "start_time": "2022-06-09T07:14:33.728079Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Proof of log-odds ratio\n",
    "Let us denote $$z= \\beta_0 + \\sum_{j = 1}^n \\beta_j X_j.$$\n",
    "\n",
    "We are to show that\n",
    "$$ \\log \\frac{P(Y=1|\\beta)}{1-P(Y=1|\\beta)} = z.$$\n",
    "We can rewrite the left hand side by splitting the fraction of the log\n",
    "\\begin{align*} \n",
    "\\log \\frac{P(Y=1|\\beta)}{1-P(Y=1|\\beta)} &=  \\log \\frac{1}{1+e^{-z}} - \\log \\frac{e^{-z}}{1+e^{-z}}\\\\ \n",
    " &=   \\log 1 - \\log e^{-z}\\\\ \n",
    " &=  - \\log e^{-z}\\\\ \n",
    " &= z\n",
    "\\end{align*}\n",
    "Note that in the second step we used that you can split again to 4 logarithms and then the log of the denominators will have opposite sign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291dec16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:14:33.751529Z",
     "start_time": "2022-06-09T07:14:33.745122Z"
    }
   },
   "source": [
    "## Interpreting the parameters\n",
    "These parameters represent the importance of features $X = (X_1,...,X_d)$ when we decide how likely $Y=1$(the individual having bad credit risk) compared to $Y=0$(the individual having good credit risk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaaab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:14:33.779556Z",
     "start_time": "2022-06-09T07:14:33.764114Z"
    }
   },
   "source": [
    "## Decision Boundry\n",
    "When $\\beta_0 + \\sum_{j = 1}^n \\beta_j X_j$ is positive, it means the individual has a bad credit risk.Otherwise, he has a good credit risk. After fitting it into sigmoid function, the threshold will be 0.5, because $expit(0) = \\log \\frac{1}{1+e^{0}} = 0.5 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-traffic",
   "metadata": {},
   "source": [
    "## Proof of log-likelihood\n",
    "Let us denote $$z_i= \\beta_0 + \\sum_{j = 1}^n \\beta_j X_{i,j}.$$\n",
    "\n",
    "If $y_i = 1$, we see that the right hand side is \n",
    "\\begin{align*} \n",
    "\\log \\frac{1}{1+e^{-z_i}}  &= \\log \\frac{e^z_i}{1+e^{z_i}}\\\\\n",
    "&= z_i - \\log 1+e^z_i\\\\\n",
    "&= y_i z_i - \\log 1+e^z_i.\n",
    "\\end{align*}\n",
    "\n",
    "If $y_i = 0$, we see that the right hand side is \n",
    "\\begin{align*} \n",
    "\\log \\frac{e^{-z_i}}{1+e^{-z_i}}  &= \\log \\frac{1}{1+e^{z_i}}\\\\\n",
    "&= 0 - \\log 1+e^{z_i}\\\\\n",
    "&= y_i z_i - \\log 1+e^{z_i}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-mexico",
   "metadata": {},
   "source": [
    "## JAX-compatible log-likelihood\n",
    "\n",
    "We are asked to calculate\n",
    "$$\\log P(y_1,...,y_m| \\beta).$$\n",
    "\n",
    "By independence of the training data points, this equals\n",
    "$$\\log \\prod_{i=1}^m P(y_i| \\beta).$$\n",
    "\n",
    "We can now use the result from previous exercise to see that \n",
    "\n",
    "\\begin{align*} \n",
    "\\log \\prod_{i=1}^m P(y_i| \\beta) &=  \\sum_{i=1}^m \\log P(y_i| \\beta) \\\\\n",
    "&= \\sum_{i=1}^m (y_i z_i - \\log 1+e^z_i)\\\\\n",
    "&= \\sum_{i=1}^m y_i z_i -  \\sum_{i=1}^m \\log 1+e^z_i.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-conference",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:32.316309Z",
     "start_time": "2022-06-13T10:13:32.221567Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Write log-likelihood function in numpy\n",
    "# def log_likelihood(beta):\n",
    "#     x_beta = np.matmul(x_train, beta)\n",
    "#     output = np.sum(y_train * x_beta - np.log(1 + np.exp(x_beta)))\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-transformation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:32.374946Z",
     "start_time": "2022-06-13T10:13:32.366514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write log-likelihood function in a JAX-compatible way\n",
    "@jit\n",
    "def log_likelihood_jax(beta):\n",
    "    x_beta = jnp.matmul(x_train, beta)\n",
    "    output = jnp.sum(y_train * x_beta - jnp.log(1 + jnp.exp(x_beta)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a53e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:13:48.324653Z",
     "start_time": "2022-06-13T10:13:32.379456Z"
    }
   },
   "outputs": [],
   "source": [
    "# To compare the speed with or without jit\n",
    "import time\n",
    "\n",
    "# simulate an array of beta\n",
    "beta = np.random.rand(25)\n",
    "jit_loglikelihood_jax = jit(log_likelihood_jax)\n",
    "%timeit jit_loglikelihood_jax(beta)\n",
    "%timeit log_likelihood_jax(beta)\n",
    "\n",
    "## There is no significant speedup, we won't use jit in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c19436e",
   "metadata": {},
   "source": [
    "## Gradient of the log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-wrestling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:14:04.084568Z",
     "start_time": "2022-06-13T10:13:48.327379Z"
    }
   },
   "outputs": [],
   "source": [
    "gradloglikelihood1 = grad(log_likelihood_jax) # without JIT\n",
    "gradloglikelihood2 = jit(grad(log_likelihood_jax)) # with JIT \n",
    "\n",
    "# for the gradient, there is a signfiicant speedup with JIT\n",
    "%timeit gradloglikelihood1(beta)\n",
    "%timeit gradloglikelihood2(beta)\n",
    "\n",
    "# After investigation, the jit implementations speeds up significantly. Use the faster implementation\n",
    "gradloglikelihood = jit(grad(log_likelihood_jax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89afd6",
   "metadata": {},
   "source": [
    "## Log-prior density evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-leone",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:14:04.144325Z",
     "start_time": "2022-06-13T10:14:04.087170Z"
    }
   },
   "outputs": [],
   "source": [
    "DIM = 25\n",
    "constant = np.pi**2 * M / (3*DIM)\n",
    "Sigma = constant * np.linalg.inv(np.matmul(x_train.T, x_train))\n",
    "\n",
    "@jit\n",
    "def log_prior(beta):\n",
    "    '''\n",
    "    Input – beta: a vector of size d+1\n",
    "    Output – log prior density: constant\n",
    "    '''\n",
    "    return multivariate_normal.logpdf(beta, mean=jnp.zeros(DIM), cov=Sigma)\n",
    "\n",
    "\n",
    "# After investigation, the jit implementations does not speed up significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a790cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:14:26.305756Z",
     "start_time": "2022-06-13T10:14:04.146691Z"
    }
   },
   "outputs": [],
   "source": [
    "log_prior1 = log_prior # without JIT\n",
    "log_prior2 = jit(log_prior) # with JIT \n",
    "%timeit log_prior1(beta)\n",
    "%timeit log_prior2(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da846cd",
   "metadata": {},
   "source": [
    "## Gradient of the log-prior evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-disability",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:14:38.878791Z",
     "start_time": "2022-06-13T10:14:26.309974Z"
    }
   },
   "outputs": [],
   "source": [
    "grad_log_prior1 = grad(log_prior)\n",
    "grad_log_prior2 = jit(grad(log_prior))\n",
    "\n",
    "%timeit grad_log_prior1(beta)\n",
    "%timeit grad_log_prior2(beta)\n",
    "\n",
    "# After investigation, jit speeds up gradient computation significantly\n",
    "grad_log_prior = grad_log_prior2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee9c7e",
   "metadata": {},
   "source": [
    "## Unnormalized log-posterior density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7221565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:14:38.942052Z",
     "start_time": "2022-06-13T10:14:38.891065Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_posterior(beta):\n",
    "    '''\n",
    "    Input – beta: a vector of size d+1\n",
    "    Output – log posterior: constant\n",
    "    '''\n",
    "    return log_prior(beta) + log_likelihood_jax(beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcdd014",
   "metadata": {},
   "source": [
    "## Gradient of log-posterior density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68d8f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:14:38.971834Z",
     "start_time": "2022-06-13T10:14:38.955734Z"
    }
   },
   "outputs": [],
   "source": [
    "def grad_log_posterior(beta):\n",
    "    '''\n",
    "    Input – beta: a vector of size d+1\n",
    "    Output – gradient step of log posterior: beta: a vector of size d+1.\n",
    "    '''\n",
    "    return grad_log_prior(beta) + gradloglikelihood(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-night",
   "metadata": {},
   "source": [
    "# Markov chain Monte Carlo sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e4500",
   "metadata": {},
   "source": [
    "## Independent Metropolis–Hastings algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc9fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:14:50.190902Z",
     "start_time": "2022-06-13T10:14:38.977826Z"
    }
   },
   "outputs": [],
   "source": [
    "# since jax's random generation is painful, we will switch to standard scipy\n",
    "import scipy\n",
    "def sample_prior():\n",
    "    return scipy.stats.multivariate_normal.rvs(mean=np.zeros(DIM), cov=Sigma)\n",
    "\n",
    "n_accept = 0\n",
    "N = 10000\n",
    "\n",
    "# Initialize the Markov Chain\n",
    "current_beta = sample_prior()\n",
    "store_beta = np.zeros((N, DIM))\n",
    "for n in range(N):\n",
    "    # sample a proposed state\n",
    "    proposed_beta = sample_prior()\n",
    "\n",
    "    # evaluate posterior density\n",
    "    log_posterior_proposed = log_posterior(proposed_beta)\n",
    "    log_posterior_current = log_posterior(current_beta)\n",
    "\n",
    "    # evaluate transition likelihood\n",
    "    log_transition_proposed = log_prior(proposed_beta)\n",
    "    log_transition_current = log_prior(current_beta)\n",
    "    \n",
    "    # log acceptance prob\n",
    "    log_accept_prob = (log_posterior_proposed + log_transition_current\n",
    "                       - log_posterior_current - log_transition_proposed)\n",
    "\n",
    "    # accept or reject\n",
    "    uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "    if np.log(uniform) < log_accept_prob:\n",
    "        current_beta = proposed_beta.copy() #accept\n",
    "        n_accept += 1\n",
    "    # store states\n",
    "    store_beta[n,:] = current_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82675dc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:14:50.417105Z",
     "start_time": "2022-06-13T10:14:50.194117Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Acceptance rate: \", n_accept/N)\n",
    "\n",
    "iteration = np.arange(1, N+1)\n",
    "plt.figure()\n",
    "plt.plot(iteration, store_beta[:,0])\n",
    "plt.plot(iteration, store_beta[:,1])\n",
    "plt.plot(iteration, store_beta[:,2])\n",
    "plt.plot(iteration, store_beta[:,3])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('beta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5c605",
   "metadata": {},
   "source": [
    "We see that the acceptance rate is 0.0017 which means we mostly reject the transition of beta and it explains why we tend to get stuck in the same beta shown in the graph. Our Markov Chain Monte-Carlo sampler is converging slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bd9ac",
   "metadata": {},
   "source": [
    "## A random walk Metropolis–Hastings algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-output",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:15:17.990751Z",
     "start_time": "2022-06-13T10:14:50.421646Z"
    }
   },
   "outputs": [],
   "source": [
    "#3.2 Experiment with the choice of sigma\n",
    "sigma_list = [0.002, 0.02, 0.2]\n",
    "for sigma in sigma_list:\n",
    "    noise_variance = np.identity(DIM) * sigma**2\n",
    "    n_accept = 0\n",
    "    current_beta = sample_prior()\n",
    "    store_beta = np.zeros((N, DIM))\n",
    "\n",
    "    for n in range(N):\n",
    "        noise = scipy.stats.multivariate_normal.rvs(mean=np.zeros(DIM),\n",
    "                                       cov=noise_variance)\n",
    "        proposed_beta = current_beta + noise\n",
    "\n",
    "        #evaluate posterior density\n",
    "        log_posterior_proposed = log_posterior(proposed_beta)\n",
    "        log_posterior_current = log_posterior(current_beta)\n",
    "\n",
    "        #accept or reject\n",
    "        log_accept_prob = log_posterior_proposed - log_posterior_current\n",
    "        uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "        if np.log(uniform) < log_accept_prob:\n",
    "            current_beta = proposed_beta.copy() #accept\n",
    "            n_accept += 1\n",
    "\n",
    "        store_beta[n,:] = current_beta\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(iteration, store_beta[:,1])\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('beta')\n",
    "    plt.title(\"Evolution of beta\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_acf(store_beta[:,1], alpha=None)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xlabel('lag')\n",
    "    plt.ylabel('autocorrelation')\n",
    "    plt.show()\n",
    "    print(f\"Acceptance probability is {n_accept/N}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e4de4",
   "metadata": {},
   "source": [
    "Based on autocorrelation graph, we can tell sigma=0.02 is best among three as it drops down to 0 faster than two others. However, it is not optimal as it's still far away from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff48bd",
   "metadata": {},
   "source": [
    "## Metropolis-adjusted Langevin algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00d739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:17:40.477860Z",
     "start_time": "2022-06-13T10:15:17.998024Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigma_list = [0.04, 0.08, 0.12]\n",
    "for sigma in sigma_list:\n",
    "    n_accept = 0\n",
    "    current_beta = sample_prior()\n",
    "    store_beta = np.zeros((N, DIM))\n",
    "    noise_variance = np.identity(DIM) * sigma**2\n",
    "    for n in range(N):\n",
    "        noise = scipy.stats.multivariate_normal.rvs(mean=np.zeros(DIM),\n",
    "                                       cov=noise_variance)\n",
    "        proposed_beta = (current_beta +\n",
    "                         0.5 * sigma**2 * grad_log_posterior(current_beta) +\n",
    "                         noise)\n",
    "\n",
    "        #evaluate posterior density\n",
    "        log_posterior_proposed = log_posterior(proposed_beta)\n",
    "        log_posterior_current = log_posterior(current_beta)\n",
    "        log_transition_proposed = multivariate_normal.logpdf(\n",
    "            proposed_beta,\n",
    "            mean=current_beta\n",
    "                 + 0.5 * sigma**2 * grad_log_posterior(current_beta),\n",
    "            cov=noise_variance)\n",
    "        log_transition_current = scipy.stats.multivariate_normal.logpdf(\n",
    "            current_beta,\n",
    "            mean=proposed_beta\n",
    "                 + 0.5 * sigma**2 * grad_log_posterior(proposed_beta),\n",
    "            cov=noise_variance)\n",
    "\n",
    "        #accept or reject\n",
    "        log_accept_prob = (log_posterior_proposed\n",
    "                           + log_transition_current\n",
    "                           - log_posterior_current\n",
    "                           - log_transition_proposed)\n",
    "        uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "        if np.log(uniform) < log_accept_prob:\n",
    "            current_beta = proposed_beta.copy() #accept\n",
    "            n_accept += 1\n",
    "\n",
    "        store_beta[n,:] = current_beta\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(iteration, store_beta[:,1])\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('beta')\n",
    "    plt.title(\"Evolution of beta\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_acf(store_beta[:,1], alpha=None)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xlabel('lag')\n",
    "    plt.ylabel('autocorrelation')\n",
    "    plt.show()\n",
    "    print(f\"Acceptance probability is {n_accept/N}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4201e",
   "metadata": {},
   "source": [
    "The sigma=0.08 seems to be the best choice. First and second choices have comparable performance on diagnostic plots. The trace plots of beta show stationarity while the second choice shows a faster drop on autocorrelation. Considering the balance between large move and decent acceptance ratio, sigma=0.08 performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13635203",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte Carlo algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-steal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:31:20.207254Z",
     "start_time": "2022-06-13T10:23:34.535748Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#3.4 : Hamiltonian Monte Carlo algorithm\n",
    "from hmc import hamiltonian_dynamics\n",
    "step_size_list = [0.01, 0.05, 0.1]\n",
    "number_step_list = [5,10]\n",
    "\n",
    "for step_size in step_size_list:\n",
    "    for number_step in number_step_list:\n",
    "        n_accept = 0\n",
    "        current_beta = sample_prior()\n",
    "        store_beta = np.zeros((N, DIM))\n",
    "        for n in range(N):\n",
    "            velocity = scipy.stats.multivariate_normal.rvs(mean=np.zeros(DIM), cov=np.identity(DIM))\n",
    "            proposed_beta, proposed_velocity = hamiltonian_dynamics(\n",
    "                current_beta, velocity, step_size, number_step, grad_log_posterior)\n",
    "            log_posterior_proposed = log_posterior(proposed_beta)\n",
    "            log_posterior_current = log_posterior(current_beta)\n",
    "            log_prob_proposed = multivariate_normal.logpdf(\n",
    "                proposed_velocity, mean=np.zeros(DIM), cov=np.identity(DIM))\n",
    "            log_prob_current = multivariate_normal.logpdf(\n",
    "                velocity, mean=np.zeros(DIM), cov=np.identity(DIM))\n",
    "\n",
    "            #accept or reject\n",
    "            log_accept_prob = (log_posterior_proposed\n",
    "                               + log_prob_proposed\n",
    "                               - log_posterior_current\n",
    "                               - log_prob_current)\n",
    "            uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "            if np.log(uniform) < log_accept_prob:\n",
    "                current_beta = proposed_beta.copy() #accept\n",
    "                n_accept += 1\n",
    "\n",
    "            store_beta[n,:] = current_beta\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iteration, store_beta[:,1])\n",
    "        plt.xlabel('iteration')\n",
    "        plt.ylabel('beta')\n",
    "        plt.title(\"Evolution of beta\")\n",
    "        plt.show()\n",
    "\n",
    "        plot_acf(store_beta[:,1], alpha=None)\n",
    "        plt.ylim([0, 1.1])\n",
    "        plt.xlabel('lag')\n",
    "        plt.ylabel('autocorrelation')\n",
    "        plt.show()\n",
    "        print(f\"Step size of {step_size} with {number_step} steps\")\n",
    "        print(f\"Acceptance probability is {n_accept/N}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26870ce8",
   "metadata": {},
   "source": [
    "Based on diagnostic plots and acceptance ratios, step size of 0.05 with 5 steps is the best setup as the autocorrelation plumps to 0 very fast and trace plot shows quite stable stationarity. Meanwhile, 0.8513 acceptance ratio is decent and 0.05 as step size can ensure a larger move in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef26b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:44:11.116199Z",
     "start_time": "2022-06-13T10:42:27.437969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a final run of the Markov chain, with the best stepsize that you found and 10 number of steps, \n",
    "# for 11, 000 iterations and discard the first 1000 iterations as burn-in.\n",
    "N = 11000\n",
    "current_beta = sample_prior()\n",
    "store_beta = np.zeros((10000, DIM))\n",
    "step_size = 0.05\n",
    "number_step = 10\n",
    "for n in range(N):\n",
    "    velocity = scipy.stats.multivariate_normal.rvs(mean=np.zeros(DIM), cov=np.identity(DIM))\n",
    "    proposed_beta, proposed_velocity = hamiltonian_dynamics(\n",
    "        current_beta, velocity, step_size, number_step, grad_log_posterior)\n",
    "    log_posterior_proposed = log_posterior(proposed_beta)\n",
    "    log_posterior_current = log_posterior(current_beta)\n",
    "    log_prob_proposed = multivariate_normal.logpdf(\n",
    "        proposed_velocity, mean=np.zeros(DIM), cov=np.identity(DIM))\n",
    "    log_prob_current = multivariate_normal.logpdf(\n",
    "        velocity, mean=np.zeros(DIM), cov=np.identity(DIM))\n",
    "\n",
    "    #accept or reject\n",
    "    log_accept_prob = (log_posterior_proposed\n",
    "                       + log_prob_proposed\n",
    "                       - log_posterior_current\n",
    "                       - log_prob_current)\n",
    "    uniform = np.random.rand(1) # sample a uniform on [0,1]\n",
    "    if np.log(uniform) < log_accept_prob:\n",
    "        current_beta = proposed_beta.copy() #accept\n",
    "    if n >= 1000:\n",
    "        store_beta[n-1000,:] = current_beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142ca50",
   "metadata": {},
   "source": [
    "# Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5956803",
   "metadata": {},
   "source": [
    "## Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-century",
   "metadata": {},
   "source": [
    "Under the integral, we can see the expit function and a probability distribution. To estimate this integral, we sample $N$ points $\\{ \\beta^{(t)}, \\space t \\in \\{1, .. , N\\} \\space \\}$ from the distribution $P(\\beta | y_1,..,y_m)$ and then add the obtained values $$\\text{expit}(z^{(t)})$$ where \n",
    "$$z^{(t)}= \\beta_0^{(t)} + \\sum_{j = 1}^n \\beta_j^{(t)} X_{j}.$$\n",
    "We give every point $\\beta^{(t)}$ a weight of $1/N$ so that the total weight is 1 and equals the integral of the probability function. This way, we obtain the method of the project. In summary, we use sampling to represent original distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45fc2f",
   "metadata": {},
   "source": [
    "## Approximated predictive probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b37926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T11:33:39.681666Z",
     "start_time": "2022-06-13T11:33:39.676253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define sigmoid\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    if x >= 0:\n",
    "        z = math.exp(-x)\n",
    "        sig = 1 / (1 + z)\n",
    "        return sig\n",
    "    else:\n",
    "        z = math.exp(x)\n",
    "        sig = z / (1 + z)\n",
    "        return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_bayes = np.mean(sigmoid(store_beta @ x_test.T), axis=0)\n",
    "y_prob_bayes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3\n",
    "y_pred_bayes = np.where(y_prob_bayes < 0.5, 0, 1)\n",
    "y_pred_bayes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ff00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.4 : Misclassification rate\n",
    "misclassification_bayes = np.mean(y_pred_bayes != y_test)\n",
    "misclassification_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d05d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 : Cost function\n",
    "def average_cost(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Input – y_test: vector with good/bad credit risk\n",
    "          – y_pred: pred values for good/bad credit risk\n",
    "    Output – average cost: constant\n",
    "    \"\"\"\n",
    "    cost_vector = np.zeros_like(y_test)\n",
    "    cost_vector[(y_pred == 0) & (y_test == 1)] = 5\n",
    "    cost_vector[(y_pred == 1) & (y_test == 0)] = 1\n",
    "    return np.mean(cost_vector)\n",
    "\n",
    "avg_cost_bayes = average_cost(y_test, y_pred_bayes)\n",
    "avg_cost_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.6 : Maximum Likelihood Estimator\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "current_beta = sample_prior()\n",
    "criterion = log_likelihood_jax\n",
    "\n",
    "beta_MLE = minimize(fun=lambda x: -log_likelihood_jax(x),\n",
    "                    x0=current_beta,\n",
    "                    jac=lambda x: -grad_log_likelihood(x)).x\n",
    "beta_MLE.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec139748",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_mle = sigmoid(x_test @ beta_MLE)\n",
    "y_pred_mle = np.where(y_prob_mle < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.7 : Misclassification rate\n",
    "misclassification_mle = np.mean(y_pred_mle != y_test)\n",
    "misclassification_mle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cost_mle = average_cost(y_test, y_pred_mle)\n",
    "avg_cost_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d66d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.8 : Prediction accuracy\n",
    "(y_prob_mle -  y_prob_bayes)\n",
    "\n",
    "f = lambda x: 5 if x ==-1 else x\n",
    "\n",
    "error = y_pred - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71307de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 : Prediction rule\n",
    "# if predict gives a probability smaller than 0.5, return 0,\n",
    "# if it is larger than 0.5, return 1\n",
    "\n",
    "def predict_all(x_test, store_beta):\n",
    "    predictions_probs = predict_all_probs(x_test, store_beta)\n",
    "    f = lambda x:  1 if (x>0.5) else 0\n",
    "\n",
    "    solutions = [ f(x) for x in predictions_probs]\n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-prime",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:23:23.865203Z",
     "start_time": "2022-06-13T10:13:27.300Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = predict_all(x_test, store_beta)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-infection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:23:23.867923Z",
     "start_time": "2022-06-13T10:13:27.303Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.4 : Misclassification rate\n",
    "misclassification = np.mean(y_pred != y_test)\n",
    "misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-patient",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:23:23.874850Z",
     "start_time": "2022-06-13T10:13:27.306Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4.5 : Cost function\n",
    "def average_cost(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Input – y_test: vector with good/bad credit risk\n",
    "          – y_pred: pred values for good/bad credit risk\n",
    "    Output – average cost: constant\n",
    "    \"\"\"\n",
    "    cost_vector = np.zeros_like(y_test)\n",
    "    cost_vector[(y_pred == 0) & (y_test == 1)] = 5\n",
    "    cost_vector[(y_pred == 1) & (y_test == 0)] = 1\n",
    "    return np.mean(cost_vector)\n",
    "\n",
    "avg_cost = average_cost(y_test, y_pred)\n",
    "avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182964e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:23:23.877598Z",
     "start_time": "2022-06-13T10:13:27.308Z"
    }
   },
   "outputs": [],
   "source": [
    "adjusted_cost =adjust(y_test, y_pred)\n",
    "\n",
    "avg_cost = average_cost(adjusted_cost)\n",
    "avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-report",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:23:23.880210Z",
     "start_time": "2022-06-13T10:13:27.311Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.6 : Maximum Likelihood Estimator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-shipping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:23:23.882484Z",
     "start_time": "2022-06-13T10:13:27.314Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.7 : Misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-honduras",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:23:23.884659Z",
     "start_time": "2022-06-13T10:13:27.316Z"
    }
   },
   "outputs": [],
   "source": [
    "#4.8 : Prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8fc5084e69b64ba3a8305ae1e10bbd89152d233669d0e5f5d19e1a4da7e5633"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
